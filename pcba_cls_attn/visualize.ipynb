{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'models loading done'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch \n",
    "\n",
    "from lgi_gt import LGI_GT \n",
    "import gnnt, parallel \n",
    "\n",
    "# LGI \n",
    "model_lgi = LGI_GT( \n",
    "            out_dim = 128, \n",
    "            gconv_dim = 384, \n",
    "            tlayer_dim = 384, \n",
    "            num_layers = 5, \n",
    "            num_heads=8, \n",
    "            local_attn_dropout=0.0, \n",
    "            global_attn_dropout=0.3, \n",
    "            local_ffn_dropout=0.3, \n",
    "            global_ffn_dropout=0.3, \n",
    "            clustering=False, \n",
    "            masked_attention=True, \n",
    "            norm='ln', \n",
    "            skip_connection='none', \n",
    "            readout='cls') \n",
    "\n",
    "model_state_lgi = torch.load(\"state/LGI.pt\")\n",
    "model_lgi.load_state_dict(model_state_lgi) \n",
    "model_lgi.eval() \n",
    "\n",
    "# GNN+Transformer \n",
    "model_gnnt = gnnt.GraphTransformer( \n",
    "            out_dim = 128, \n",
    "            gconv_dim = 384, \n",
    "            tlayer_dim = 384, \n",
    "            num_layers = 5, \n",
    "            num_heads=8, \n",
    "            local_attn_dropout=0.0, \n",
    "            global_attn_dropout=0.3, \n",
    "            local_ffn_dropout=0.3, \n",
    "            global_ffn_dropout=0.3, \n",
    "            clustering=False, \n",
    "            masked_attention=True, \n",
    "            norm='ln', \n",
    "            skip_connection='none', \n",
    "            readout='cls') \n",
    "\n",
    "model_state_gnnt = torch.load(\"state/GNNT.pt\")\n",
    "model_gnnt.load_state_dict(model_state_gnnt) \n",
    "model_gnnt.eval() \n",
    "\n",
    "# parallel \n",
    "model_parallel = parallel.GraphTransformer( \n",
    "            out_dim = 128, \n",
    "            gconv_dim = 384, \n",
    "            tlayer_dim = 384, \n",
    "            num_layers = 5, \n",
    "            num_heads=8, \n",
    "            local_attn_dropout=0.0, \n",
    "            global_attn_dropout=0.3, \n",
    "            local_ffn_dropout=0.3, \n",
    "            global_ffn_dropout=0.3, \n",
    "            clustering=False, \n",
    "            masked_attention=True, \n",
    "            norm='ln', \n",
    "            skip_connection='none', \n",
    "            readout='cls') \n",
    "\n",
    "model_state_parallel = torch.load(\"state/Parallel.pt\")\n",
    "model_parallel.load_state_dict(model_state_parallel) \n",
    "model_parallel.eval() \n",
    "\n",
    "\"\"\"models loading done\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "43793"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from torch_geometric.loader import DataLoader # for pyg == 2.0.4 \n",
    "from ogb.graphproppred import PygGraphPropPredDataset \n",
    "\n",
    "dataset = PygGraphPropPredDataset(name='ogbg-molpcba', root='.') \n",
    "split_idx = dataset.get_idx_split() \n",
    "# val_loader = DataLoader(dataset[split_idx[\"valid\"]], batch_size=1, shuffle=False) \n",
    "val_dataset = dataset[split_idx['valid']] \n",
    "len(val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from visualize_draw import draw "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({5, 6, 7, 15},\n",
       " [5,\n",
       "  7,\n",
       "  5,\n",
       "  5,\n",
       "  5,\n",
       "  5,\n",
       "  5,\n",
       "  5,\n",
       "  5,\n",
       "  5,\n",
       "  5,\n",
       "  5,\n",
       "  5,\n",
       "  5,\n",
       "  7,\n",
       "  5,\n",
       "  7,\n",
       "  6,\n",
       "  7,\n",
       "  5,\n",
       "  5,\n",
       "  5,\n",
       "  5,\n",
       "  15,\n",
       "  5,\n",
       "  5,\n",
       "  6])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_idx = 6\n",
    "\n",
    "# 6, 2023, 2022, 10000, 996, 5555, 22222, 777 \n",
    "\n",
    "data = val_dataset[data_idx] \n",
    "data.batch = torch.zeros(data.num_nodes, dtype=torch.int64) \n",
    "data.num_graphs = 1 \n",
    "\n",
    "# model.get_clf_attn(batch, 1)[0] # 1-st layer clf attn, 1-st head \n",
    "clf_attn_lgi = model_lgi.get_clf_attn(data, 5).mean(dim=0) # the last layer clf attn, mean of all heads \n",
    "clf_attn_gnnt = model_gnnt.get_clf_attn(data, 5).mean(dim=0) \n",
    "clf_attn_parallel = model_parallel.get_clf_attn(data, 5).mean(dim=0) \n",
    "\n",
    "data.tag = data.x[:, 0] \n",
    "data.attn_lgi = clf_attn_lgi \n",
    "data.attn_gnnt = clf_attn_gnnt \n",
    "data.attn_parallel = clf_attn_parallel \n",
    "\n",
    "draw(data) \n",
    "\n",
    "set(data.tag.tolist()), data.tag.tolist() # 5: C, 6: N, 7: O, 15: S "
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "341944e75731248762244a78abc7aab60acf269663e2c4583244c63406468318"
  },
  "kernelspec": {
   "display_name": "Python 3.9.13 64-bit ('genv': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
